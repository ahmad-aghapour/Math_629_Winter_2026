{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1J_CIK_wn60LE181VQEdJMHX5Xc3WzaHb","authorship_tag":"ABX9TyMjIPLaQkR3RRzUbyOl/U+o"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Linear Regression"],"metadata":{"id":"6j5fnI8eGtBd"}},{"cell_type":"code","source":["import numpy as np\n","\n","def softmax(z):\n","    # z: (batch, K)\n","    z = z - np.max(z, axis=1, keepdims=True)  # stability\n","    exp_z = np.exp(z)\n","    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n","\n","def cross_entropy_loss(probs, y):\n","    \"\"\"\n","    probs: (n, K) softmax probabilities\n","    y: (n,) integer class labels in {0..K-1}\n","    \"\"\"\n","    n = y.shape[0]\n","    return -np.mean(np.log(probs[np.arange(n), y] + 1e-12))\n","\n","def solve_softmax_regression_via_gradient_descent(\n","    X, y,\n","    lr=0.001,\n","    n_steps=10000,\n","    tol=1e-6,\n","    batch_size=512,\n","    reg=0.0,          # L2 regularization strength (0 = none)\n","    seed=None\n","):\n","    \"\"\"\n","    X: (n, d)\n","    y: (n,) integer labels {0..K-1}\n","    Returns: loss, W, b, loss_history\n","    \"\"\"\n","    if seed is not None:\n","        np.random.seed(seed)\n","\n","    n, d = X.shape\n","    K = int(np.max(y)) + 1  # number of classes\n","\n","    W = 0.01 * np.random.randn(d, K)\n","    b = np.zeros(K)\n","\n","    loss_history = []\n","    prev_loss = np.inf\n","    patience  = 0\n","    for step in range(n_steps):\n","        # mini-batch\n","        idx = np.random.choice(n, size=min(batch_size, n), replace=False)\n","        Xb = X[idx]                 # (m, d)\n","        yb = y[idx]                 # (m,)\n","        m = Xb.shape[0]\n","\n","        # forward\n","        logits = Xb @ W + b         # (m, K)\n","        probs = softmax(logits)     # (m, K)\n","\n","        # loss (batch)\n","\n","        loss = cross_entropy_loss(probs, yb) + 0.5 * reg * np.sum(W * W)\n","        loss_history.append(loss)\n","\n","        # gradient: (probs - one_hot)\n","        grad_logits = probs\n","        grad_logits[np.arange(m), yb] -= 1.0\n","        grad_logits /= m            # average\n","\n","        gradW = Xb.T @ grad_logits + reg * W   # (d, K)\n","        gradb = np.sum(grad_logits, axis=0)    # (K,)\n","\n","        # update\n","        W -= lr * gradW\n","        b -= lr * gradb\n","\n","        # stopping by small improvement\n","\n","        if abs(prev_loss - loss) < tol:\n","            patience += 1\n","            if patience == 5:\n","                break\n","        else:\n","            patience = 0\n","        prev_loss = loss\n","\n","    return loss_history[-1], W, b, loss_history\n","\n","def predict_softmax(X, W, b):\n","    probs = softmax(X @ W + b)\n","    return np.argmax(probs, axis=1)\n"],"metadata":{"id":"VQ7s8Iq0K6KM","executionInfo":{"status":"ok","timestamp":1769180443271,"user_tz":300,"elapsed":34,"user":{"displayName":"Ahmad Aghapour","userId":"10024107043211715538"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["# Import necessary libraries\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.datasets import fetch_openml\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, classification_report\n","from sklearn.preprocessing import StandardScaler\n","\n","# Load the MNIST dataset\n","mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n","X, y = mnist.data, mnist.target\n","\n"],"metadata":{"id":"3uo-m3brGvFt","executionInfo":{"status":"ok","timestamp":1769180013623,"user_tz":300,"elapsed":7533,"user":{"displayName":"Ahmad Aghapour","userId":"10024107043211715538"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","rng = np.random.RandomState(42)\n","indices = rng.choice(len(X), size=5, replace=False)\n","fig, axes = plt.subplots(1, 5, figsize=(10, 2))\n","for ax, idx in zip(axes, indices):\n","    ax.imshow(X[idx].reshape(28, 28), cmap='gray', interpolation='nearest')\n","    ax.set_title(str(y[idx]))\n","    ax.axis('off')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"SI7GgKRrSGUR","colab":{"base_uri":"https://localhost:8080/","height":217},"executionInfo":{"status":"ok","timestamp":1769180013870,"user_tz":300,"elapsed":244,"user":{"displayName":"Ahmad Aghapour","userId":"10024107043211715538"}},"outputId":"0656ecd6-326e-4586-a385-80ec1fdcfc64"},"execution_count":34,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x200 with 5 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["# Convert target labels to integers\n","y = y.astype(int)\n","n_lable = len(set(y))\n","print(f\"Number of labels: {n_lable}\")\n"],"metadata":{"id":"9SRTSzyvKOld","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769180013871,"user_tz":300,"elapsed":6,"user":{"displayName":"Ahmad Aghapour","userId":"10024107043211715538"}},"outputId":"1241a44a-3573-49a1-9620-d5e4e5a24774"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of labels: 10\n"]}]},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"],"metadata":{"id":"g3A6HnkcUFXF","executionInfo":{"status":"ok","timestamp":1769180015199,"user_tz":300,"elapsed":1327,"user":{"displayName":"Ahmad Aghapour","userId":"10024107043211715538"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["final_loss , W , b ,loss_hist =   solve_softmax_regression_via_gradient_descent(X_train , y_train ,n_steps =1000)"],"metadata":{"id":"gb5It4uMT_pj","executionInfo":{"status":"ok","timestamp":1769180078870,"user_tz":300,"elapsed":17849,"user":{"displayName":"Ahmad Aghapour","userId":"10024107043211715538"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["predict  = predict_softmax(X_test , W , b)\n","acc = accuracy_score(y_test , predict)\n","print(f\"Accuracy: {acc}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l9s_c17hU8F4","executionInfo":{"status":"ok","timestamp":1769180079004,"user_tz":300,"elapsed":130,"user":{"displayName":"Ahmad Aghapour","userId":"10024107043211715538"}},"outputId":"e679fb22-ec06-4aae-b19d-ad0eaf5680d2"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.8804285714285714\n"]}]},{"cell_type":"code","source":["random_feature_generator  = np.random.RandomState(12).normal(size=(X_train.shape[1], 4096)).astype(np.float32)\n","Z_train, Z_test = np.maximum(0, X_train.dot(random_feature_generator)), np.maximum(0, X_test.dot(random_feature_generator))\n","\n","final_loss , W , b ,loss_hist =   solve_softmax_regression_via_gradient_descent(Z_train , y_train ,n_steps = 1000, batch_size = 1024)\n","\n","\n","predict  = predict_softmax(Z_test , W , b)\n","acc = accuracy_score(y_test , predict)\n","print(f\"Accuracy: {acc}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sAjwjwbHXIN0","executionInfo":{"status":"ok","timestamp":1769180714288,"user_tz":300,"elapsed":61229,"user":{"displayName":"Ahmad Aghapour","userId":"10024107043211715538"}},"outputId":"0526ddfe-f464-48c3-e47a-3094fb25f402"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.9477857142857142\n"]}]},{"cell_type":"code","source":["from sklearn.linear_model import SGDClassifier\n","\n","W = np.random.RandomState(42).normal(size=(X_train.shape[1], 4096)).astype(np.float32)\n","Z_train, Z_test = np.maximum(0, X_train.dot(W)), np.maximum(0, X_test.dot(W))\n","\n","clf = SGDClassifier(\n","    loss='log_loss',           # logistic regression\n","    max_iter=1000,             # upper cap\n","    tol=1e-3,                  # stop when improvement < tol\n","    early_stopping=True,       # use an internal validation split and stop early\n","    n_iter_no_change=5,\n","    validation_fraction=0.1,   # fraction of train set used for early stopping\n","    random_state=42,\n","    verbose=0\n",")\n","\n","clf.fit(Z_train, y_train)\n","\n","# Evaluate\n","y_pred = clf.predict(Z_test)"],"metadata":{"id":"quCEO4i4Qvse","executionInfo":{"status":"ok","timestamp":1769180328176,"user_tz":300,"elapsed":54426,"user":{"displayName":"Ahmad Aghapour","userId":"10024107043211715538"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["print(\"Accuracy (SGDClassifier):\", accuracy_score(y_test, y_pred))\n"],"metadata":{"id":"nO0rBhf7RDEH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769180331533,"user_tz":300,"elapsed":14,"user":{"displayName":"Ahmad Aghapour","userId":"10024107043211715538"}},"outputId":"3d4ab476-e66c-4c64-e10a-f22ea21d4bbf"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy (SGDClassifier): 0.9680714285714286\n"]}]},{"cell_type":"markdown","source":["# Ridge and Lasso"],"metadata":{"id":"bQyDM1NwSw1C"}},{"cell_type":"markdown","source":["\n","\n","## **1. Introduction**\n","Linear regression is a fundamental technique in **quantitative finance** for modeling relationships between financial indicators and future stock returns. In this lecture, we apply:\n","\n","- **Ordinary Least Squares (OLS)** to estimate feature importance.\n","- **Ridge Regression** to control overfitting.\n","- **Lasso Regression** to perform feature selection.\n","\n","The objective is to predict **next-day returns** using technical indicators.\n","\n","---\n","\n","## **2. Define Features and Target**\n","We define our independent variables (**X**) as technical indicators and past returns. The dependent variable (**y**) is the **next-day return**.\n","\n","\n"],"metadata":{"id":"5Pm8iBnNx5wn"}},{"cell_type":"markdown","source":["## Ridge regression"],"metadata":{"id":"uM9gpXw4zqDc"}},{"cell_type":"markdown","source":["## Why Lasso Tends to Produce More Zeros Than Ridge\n","\n","### Regularization in Lasso and Ridge\n","\n","- **Lasso Regression (L1 Regularization):**\n","  - Uses the L1 norm of the coefficients as the penalty:  $$\\text{Penalty}_{\\text{Lasso}} = \\lambda \\sum_{j=1}^{p} |\\beta_j| $$  \n","\n","  - The gradient (or subgradient) with respect to a coefficient $\\beta_j$ is given by:\n","\n","    $$ \\frac{\\partial}{\\partial \\beta_j} \\lambda |\\beta_j| = \\lambda \\cdot \\text{sgn}(\\beta_j) $$\n","\n","    where $ \\text{sgn}(\\beta_j) $ is the sign function:\n","\n","    - $+1$ if $\\beta_j > 0$\n","    - $-1$ if $\\beta_j < 0$\n","    - Undefined at $\\beta_j = 0$ (handled via subgradient methods)\n","  - **Key Point:** The gradient is **constant in magnitude** (i.e., it does not depend on the size of $\\beta_j$). This means that even if $\\beta_j$ is very small, the same constant force is applied to push it towards zero.\n","\n","- **Ridge Regression (L2 Regularization):**\n","  - Uses the L2 norm squared of the coefficients as the penalty: $$ \\text{Penalty}_{\\text{Ridge}} = \\lambda \\sum_{j=1}^{p} \\beta_j^2 $$\n","   \n","\n","  - The gradient with respect to $\\beta_j$ is:\n","  $$ \\frac{\\partial}{\\partial \\beta_j} \\lambda \\beta_j^2 = 2\\lambda \\beta_j  $$\n","    \n","  - **Key Point:** The gradient is **proportional to the magnitude** of $\\beta_j$. Larger coefficients are penalized more strongly, but small coefficients receive a very small push towards zero.\n","\n","### How This Affects Sparsity\n","\n","- **Lasso Regression:**\n","  - Because the gradient (or penalty force) is constant regardless of the magnitude of $\\beta_j$ , even small coefficients can be driven exactly to zero.\n","  - This results in a **sparse solution**, where many coefficients are exactly zero, effectively performing feature selection.\n","\n","- **Ridge Regression:**\n","  - Since the gradient depends on $\\beta_j$'s magnitude, the shrinkage effect is **more pronounced on larger coefficients**.\n","  - Small coefficients are shrunk less aggressively, so they rarely become exactly zero.\n","  - This results in a model where all predictors are retained (although their contributions might be small), leading to **non-sparse** solutions.\n"],"metadata":{"id":"yqhmcDwh1iBO"}}]}