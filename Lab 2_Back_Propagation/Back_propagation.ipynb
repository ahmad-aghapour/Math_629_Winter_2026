{"cells":[{"cell_type":"markdown","id":"ad11ae8a","metadata":{"id":"ad11ae8a"},"source":["# Backprop from scratch â€” Placeholder (Student TODO)\n","\n","Implement the `TODO` sections in order:\n","1. Losses (MSE, CrossEntropy)\n","2. Layer forward/backward\n","3. step() with **L2 regularization on weights only**\n","4. train() mini-batch SGD + validation logging\n"]},{"cell_type":"code","execution_count":1,"id":"97f9b44d","metadata":{"executionInfo":{"elapsed":37,"status":"ok","timestamp":1770393533570,"user":{"displayName":"Ahmad Aghapour","userId":"10024107043211715538"},"user_tz":300},"id":"97f9b44d"},"outputs":[],"source":["import numpy as np\n","\n","EPS = 1e-12\n","\n","def accuracy_from_logits(probs, y_onehot):\n","    pred = np.argmax(probs, axis=1)\n","    true = np.argmax(y_onehot, axis=1)\n","    return np.mean(pred == true)\n","class Loss:\n","    def loss(self, x, y):\n","        raise NotImplementedError()\n","\n","    def gradient(self, x, y):\n","        raise NotImplementedError()\n","\n","class MSE(Loss):\n","    # Mean over ALL elements (batch * out_dim)\n","    def loss(self, x, y):\n","        return np.mean(np.power(x - y, 2))\n","\n","    def gradient(self, x, y):\n","        # derivative of mean((x-y)^2) is 2*(x-y)/x.size\n","        return 2 * (x - y) / x.size\n","\n","class CrossEntropy(Loss):\n","    # Mean over samples, sum over classes: mean( -sum(y*log(p)) )\n","    def loss(self, x, y):\n","        x = np.clip(x, EPS, 1.0)\n","        return -np.mean(np.sum(y * np.log(x), axis=1))\n","\n","    def gradient(self, x, y):\n","        x = np.clip(x, EPS, 1.0)\n","        n = y.shape[0]\n","        return -(y / x) / n\n","\n","class Layer:\n","    def __init__(self, input_dim, output_dim, non_linearity=None) -> None:\n","        self.in_dim = input_dim\n","        self.out_dim = output_dim\n","        self.non_linearity = non_linearity\n","\n","        self.output = None\n","        self.input = None\n","        self.grad_weight = None\n","        self.grad_bias = None\n","\n","        # He init is fine for ReLU-ish nets\n","        self.weights = np.random.randn(self.in_dim, self.out_dim) * np.sqrt(2 / self.in_dim)\n","        self.bias = np.zeros(self.out_dim)\n","\n","    def forward(self, x):\n","        self.input = x\n","        z = x @ self.weights + self.bias\n","\n","        if self.non_linearity is None:\n","            self.output = z\n","        elif self.non_linearity == \"relu\":\n","            self.output = np.maximum(0, z)\n","        elif self.non_linearity == \"tanh\":\n","            self.output = np.tanh(z)\n","        elif self.non_linearity == \"soft_max\":\n","            # stable softmax\n","            z = z - np.max(z, axis=1, keepdims=True)\n","            exp_z = np.exp(z)\n","            self.output = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n","        else:\n","            raise ValueError(f\"Unknown non_linearity: {self.non_linearity}\")\n","\n","        return self.output\n","\n","    def backward(self, gradients):\n","        # gradients is dL/d(output_of_this_layer)\n","        if self.non_linearity is None:\n","            grad = gradients\n","        elif self.non_linearity == \"relu\":\n","            grad = (self.output > 0) * gradients\n","        elif self.non_linearity == \"tanh\":\n","            grad = (1 - self.output ** 2) * gradients\n","        elif self.non_linearity == \"soft_max\":\n","            # Jacobian-vector product for softmax\n","            # dL/dz = s * (g - sum(g*s))\n","            s = self.output\n","            grad = s * (gradients - np.sum(gradients * s, axis=1, keepdims=True))\n","        else:\n","            raise ValueError(f\"Unknown non_linearity: {self.non_linearity}\")\n","\n","        # parameter gradients\n","        self.grad_weight = self.input.T @ grad\n","        self.grad_bias = np.sum(grad, axis=0)\n","\n","        # propagate to previous layer\n","        grad_to_input = grad @ self.weights.T\n","        return grad_to_input\n","\n","class NeuralNetwork:\n","    def __init__(self, in_dim, layers, out_dim, loss) -> None:\n","        self.layers_size = layers\n","        self.in_dim = in_dim\n","        self.out_dim = out_dim\n","        self.loss = loss\n","        self.layers = []\n","        prev = self.in_dim\n","        for h in self.layers_size:\n","            self.layers.append(Layer(prev, h, non_linearity='relu'))\n","            prev = h\n","\n","        if loss == \"mse\":\n","            self.layers.append(Layer(prev, self.out_dim, non_linearity=None))\n","            self.loss = MSE()\n","        elif loss == \"cross_entropy\":\n","            self.layers.append(Layer(prev, self.out_dim, non_linearity=\"soft_max\"))\n","            self.loss = CrossEntropy()\n","        else:\n","            raise ValueError(\"loss must be 'mse' or 'cross_entropy'\")\n","\n","    def forward(self, x):\n","        for layer in self.layers:\n","            x = layer.forward(x)\n","        return x\n","\n","    def backward(self, gradients):\n","        for layer in reversed(self.layers):\n","            gradients = layer.backward(gradients)\n","\n","    def step(self, lr):\n","        for layer in self.layers:\n","            layer.weights -= lr * layer.grad_weight\n","            layer.bias -= lr * layer.grad_bias\n","\n","    def train(self, x, y, epochs, lr, batch_size=64, verbose_every=1 , x_val = None , y_val = None):\n","        n = x.shape[0]\n","\n","        for epoch in range(epochs):\n","\n","            #suffhle\n","\n","            for start in range(0, n, batch_size):\n","                end = min(start + batch_size, n)\n","                xb = x_shuf[start:end]\n","                yb = y_shuf[start:end]\n","\n","               # implement training section\n","\n","            if (epoch + 1) % verbose_every == 0:\n","\n","                out_string = f\"Epoch {epoch+1}/{epochs} - Train_Loss: {total_loss / n_batches:.4f} \"\n","\n","                if x_val is not None  :\n","                  y_pred = self.forward(x_val)\n","                  loss = self.loss.loss(y_pred, y_val)\n","                  out_string += f\"Validation_Loss: {loss:.4f} \"\n","                  if isinstance(self.loss , CrossEntropy) :\n","                    train_accurcy = accuracy_from_logits(y_pred ,y_val )\n","                    out_string += f\"Validation_acc: {train_accurcy:.4f}\"\n","\n","                print(out_string)\n","\n","\n","\n","    def predict(self, x):\n","        return self.forward(x)\n"]},{"cell_type":"code","execution_count":2,"id":"1437d286","metadata":{"id":"1437d286","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770393537562,"user_tz":300,"elapsed":25,"user":{"displayName":"Ahmad Aghapour","userId":"10024107043211715538"}},"outputId":"b2c33443-9ede-41d1-969f-877762ee9920"},"outputs":[{"output_type":"stream","name":"stdout","text":["pred shape: (5, 3)\n","Layer 0: W (4, 8), dW (4, 8), b (8,), db (8,)\n","Layer 1: W (8, 8), dW (8, 8), b (8,), db (8,)\n","Layer 2: W (8, 3), dW (8, 3), b (3,), db (3,)\n","loss: 3.0100516077073376\n"]}],"source":["# Sanity check\n","X = np.random.randn(5, 4)              # batch=5, in_dim=4\n","y = np.random.randn(5, 3)              # regression target for mse (batch=5, out_dim=3)\n","\n","net = NeuralNetwork(in_dim=4, layers=[8, 8], out_dim=3, loss=\"mse\")\n","pred = net.forward(X)\n","print(\"pred shape:\", pred.shape)\n","\n","loss_val = net.loss.loss(pred, y)\n","grads = net.loss.gradient(pred, y)\n","net.backward(grads)\n","\n","for i, layer in enumerate(net.layers):\n","    print(f\"Layer {i}: W {layer.weights.shape}, dW {layer.grad_weight.shape}, b {layer.bias.shape}, db {layer.grad_bias.shape}\")\n","print(\"loss:\", loss_val)\n"]},{"cell_type":"code","execution_count":3,"id":"-fq3SjMefS9I","metadata":{"id":"-fq3SjMefS9I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770393545241,"user_tz":300,"elapsed":25,"user":{"displayName":"Ahmad Aghapour","userId":"10024107043211715538"}},"outputId":"14387ee5-1bfa-4d82-bdcb-ee62776026cd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loss: 0.9884942245967632\n","error: 2.195265959961184e-11\n","(Rule of thumb: <1e-4 is usually good for this tiny check.)\n"]}],"source":["def one_hot(y, num_classes):\n","    y = np.asarray(y).astype(int)\n","    oh = np.zeros((y.shape[0], num_classes))\n","    oh[np.arange(y.shape[0]), y] = 1.0\n","    return oh\n","\n","\n","\n","def numerical_grad_weight(net, X, Y, layer_index=0, eps=1e-5):\n","    # compute numerical gradient for W in net.layers[layer_index]\n","    layer = net.layers[layer_index]\n","    W = layer.weights\n","    numgrad = np.zeros_like(W)\n","\n","    # baseline\n","    base_pred = net.forward(X)\n","    base_loss = net.loss.loss(base_pred, Y)\n","\n","    for i in range(W.shape[0]):\n","        for j in range(W.shape[1]):\n","            old = W[i, j]\n","\n","            W[i, j] = old + eps\n","            lp = net.loss.loss(net.forward(X), Y)\n","\n","            W[i, j] = old - eps\n","            lm = net.loss.loss(net.forward(X), Y)\n","\n","            numgrad[i, j] = (lp - lm) / (2 * eps)\n","            W[i, j] = old\n","\n","    return numgrad, base_loss\n","\n","# small classification net for the check\n","Xc = np.random.randn(6, 3)\n","yc = one_hot(np.random.randint(0, 2, size=6), 2)\n","\n","netc = NeuralNetwork(in_dim=3, layers=[5], out_dim=2, loss=\"cross_entropy\")\n","\n","# analytic grad\n","pred = netc.forward(Xc)\n","loss_val = netc.loss.loss(pred, yc)\n","grads = netc.loss.gradient(pred, yc)\n","netc.backward(grads)\n","analytic = netc.layers[0].grad_weight.copy()\n","\n","# numerical grad\n","num, _ = numerical_grad_weight(netc, Xc, yc, layer_index=0, eps=1e-5)\n","\n","err = np.linalg.norm(analytic - num)\n","print(\"Loss:\", loss_val)\n","print(\"error:\", err)\n","\n"]},{"cell_type":"code","execution_count":null,"id":"CyhuYTYafX63","metadata":{"id":"CyhuYTYafX63"},"outputs":[],"source":["import pandas as pd\n","\n","df = pd.read_csv(\"/content/sample_data/california_housing_train.csv\")\n","\n","X , y = df.iloc[: , :-1].values , df.iloc[: , -1 : ].values /10000\n","\n","X = (X  - np.mean(X , axis= 0 )) / np.std(X , axis= 0 )\n","perm = np.random.permutation(X.shape[0])\n","X_shuf, y_shuf = X[perm], y[perm]\n","\n","split = int(X.shape[0] * 0.8)\n","X_train , y_train = X_shuf[:split] , y_shuf[:split]\n","X_val , y_val = X_shuf[split:] , y_shuf[split:]\n","\n","model = NeuralNetwork(X.shape[1] , [16  , 32 , 32 , 16] , y.shape[1] , \"mse\")\n","\n","model.train(X_train , y_train , 1000 , 0.001 , 64 , verbose_every=1 , x_val = X_val , y_val = y_val)\n","\n"]},{"cell_type":"code","execution_count":null,"id":"jHiPdc0ofZdH","metadata":{"id":"jHiPdc0ofZdH"},"outputs":[],"source":["\n","df = pd.read_csv(\"/content/sample_data/mnist_train_small.csv\")\n","\n","X , y = df.iloc[: ,1 : ].values , df.iloc[: , 0 ].values\n","\n","n_class = len(set(y))\n","y = one_hot(y , n_class)\n","\n","perm = np.random.permutation(X.shape[0])\n","X_shuf, y_shuf = X[perm], y[perm]\n","\n","split = int(X.shape[0] * 0.8)\n","X_train , y_train = X_shuf[:split] , y_shuf[:split]\n","X_val , y_val = X_shuf[split:] , y_shuf[split:]"]},{"cell_type":"code","execution_count":null,"id":"B4677SxafbDv","metadata":{"id":"B4677SxafbDv"},"outputs":[],"source":["model = NeuralNetwork(X.shape[1] , [16  , 32 , 32 , 16] , y.shape[1] , \"cross_entropy\")\n","\n","model.train(X , y , 1000 , 0.001, 64 , verbose_every=1 , x_val = X_val , y_val = y_val)\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.x"}},"nbformat":4,"nbformat_minor":5}