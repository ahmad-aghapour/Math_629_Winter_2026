{"cells":[{"cell_type":"markdown","id":"ad11ae8a","metadata":{"id":"ad11ae8a"},"source":["# Backprop from scratch â€” Placeholder (Student TODO)\n","\n","Implement the `TODO` sections in order:\n","1. Losses (MSE, CrossEntropy)\n","2. Layer forward/backward\n","3. step() with **L2 regularization on weights only**\n","4. train() mini-batch SGD + validation logging\n"]},{"cell_type":"code","execution_count":null,"id":"97f9b44d","metadata":{"id":"97f9b44d"},"outputs":[],"source":["import numpy as np\n","\n","EPS = 1e-12\n","\n","def accuracy_from_logits(probs, y_onehot):\n","    pred = np.argmax(probs, axis=1)\n","    true = np.argmax(y_onehot, axis=1)\n","    return np.mean(pred == true)\n","\n","\n","# =========================\n","# 1) Losses (TODO)\n","# =========================\n","class Loss:\n","    def loss(self, x, y):\n","        raise NotImplementedError()\n","\n","    def gradient(self, x, y):\n","        raise NotImplementedError()\n","\n","\n","class MSE(Loss):\n","    \"\"\"Mean squared error (mean over all elements).\"\"\"\n","    def loss(self, x, y):\n","        # TODO:\n","        raise NotImplementedError()\n","\n","    def gradient(self, x, y):\n","        # TODO: derivative of mean((x-y)^2) w.r.t x\n","        raise NotImplementedError()\n","\n","\n","class CrossEntropy(Loss):\n","    \"\"\"Cross-entropy for one-hot y and probability predictions x (softmax output).\"\"\"\n","    def loss(self, x, y):\n","        # TODO:\n","        raise NotImplementedError()\n","\n","    def gradient(self, x, y):\n","        # TODO:\n","        raise NotImplementedError()\n","\n","\n","# =========================\n","# 2) Dense Layer (TODO)\n","# =========================\n","class Layer:\n","    def __init__(self, input_dim, output_dim, non_linearity=None) -> None:\n","        self.in_dim = input_dim\n","        self.out_dim = output_dim\n","        self.non_linearity = non_linearity\n","\n","        self.output = None\n","        self.input = None\n","        self.grad_weight = None\n","        self.grad_bias = None\n","\n","        # He init is fine for ReLU-ish nets\n","        self.weights = np.random.randn(self.in_dim, self.out_dim) * np.sqrt(2 / self.in_dim)\n","        self.bias = np.zeros(self.out_dim)\n","\n","    def forward(self, x):\n","        \"\"\"Return layer output. Store what you need for backward.\"\"\"\n","        self.input = x\n","        z = x @ self.weights + self.bias\n","\n","        # TODO: implement activations\n","        # - None: output = z\n","        # - \"relu\": output = max(0, z)\n","        # - \"tanh\": output = tanh(z)\n","        # - \"soft_max\": stable softmax over axis=1\n","        raise NotImplementedError()\n","\n","    def backward(self, gradients):\n","        \"\"\"\n","        gradients is dL/d(output_of_this_layer).\n","        Return gradient to previous layer: dL/d(input_of_this_layer).\n","        \"\"\"\n","        # TODO: convert dL/d(output) -> dL/dz depending on activation\n","        # - None: grad = gradients\n","        # - relu: grad = (output > 0) * gradients\n","        # - tanh: grad = (1 - output^2) * gradients\n","        # - softmax: use JVP: s * (g - sum(g*s))\n","\n","        raise NotImplementedError()\n","\n","\n","# =========================\n","# 3) Network + Training (TODO)\n","# =========================\n","class NeuralNetwork:\n","    def __init__(self, in_dim, layers, out_dim, loss) -> None:\n","        self.layers_size = layers\n","        self.in_dim = in_dim\n","        self.out_dim = out_dim\n","        self.loss_name = loss\n","\n","        self.layers = []\n","        prev = self.in_dim\n","        for h in self.layers_size:\n","            self.layers.append(Layer(prev, h, non_linearity='relu'))\n","            prev = h\n","\n","        if loss == \"mse\":\n","            self.layers.append(Layer(prev, self.out_dim, non_linearity=None))\n","            self.loss = MSE()\n","        elif loss == \"cross_entropy\":\n","            self.layers.append(Layer(prev, self.out_dim, non_linearity=\"soft_max\"))\n","            self.loss = CrossEntropy()\n","        else:\n","            raise ValueError(\"loss must be 'mse' or 'cross_entropy'\")\n","\n","    def forward(self, x):\n","        # TODO (optional): students can implement this too\n","        for layer in self.layers:\n","            x = layer.forward(x)\n","        return x\n","\n","    def backward(self, gradients):\n","\n","    def step(self, lr, l2_lambda=0.0):\n","        \"\"\"\n","        Update parameters.\n","        TODO: add L2 regularization (weight decay) ONLY on weights (not biases).\n","\n","        Standard: W <- W - lr * (dW + l2_lambda * W)\n","        \"\"\"\n","        for layer in self.layers:\n","\n","            raise NotImplementedError()\n","\n","    def train(self, x, y, epochs, lr, batch_size=64, verbose_every=1, x_val=None, y_val=None, l2_lambda=0.0):\n","        \"\"\"\n","        Mini-batch SGD training with optional validation logging.\n","\n","        TODO:\n","        - shuffle each epoch\n","        - loop over mini-batches\n","        - forward -> loss -> backward -> step (with L2)\n","        - print losses (and validation acc for cross-entropy)\n","        \"\"\"\n","        raise NotImplementedError()\n","\n","    def predict(self, x):\n","        return self.forward(x)\n"]},{"cell_type":"code","execution_count":null,"id":"1437d286","metadata":{"id":"1437d286"},"outputs":[],"source":["# Sanity check\n","X = np.random.randn(5, 4)              # batch=5, in_dim=4\n","y = np.random.randn(5, 3)              # regression target for mse (batch=5, out_dim=3)\n","\n","net = NeuralNetwork(in_dim=4, layers=[8, 8], out_dim=3, loss=\"mse\")\n","pred = net.forward(X)\n","print(\"pred shape:\", pred.shape)\n","\n","loss_val = net.loss.loss(pred, y)\n","grads = net.loss.gradient(pred, y)\n","net.backward(grads)\n","\n","for i, layer in enumerate(net.layers):\n","    print(f\"Layer {i}: W {layer.weights.shape}, dW {layer.grad_weight.shape}, b {layer.bias.shape}, db {layer.grad_bias.shape}\")\n","print(\"loss:\", loss_val)\n"]},{"cell_type":"code","source":["def one_hot(y, num_classes):\n","    y = np.asarray(y).astype(int)\n","    oh = np.zeros((y.shape[0], num_classes))\n","    oh[np.arange(y.shape[0]), y] = 1.0\n","    return oh\n","\n","\n","\n","def numerical_grad_weight(net, X, Y, layer_index=0, eps=1e-5):\n","    # compute numerical gradient for W in net.layers[layer_index]\n","    layer = net.layers[layer_index]\n","    W = layer.weights\n","    numgrad = np.zeros_like(W)\n","\n","    # baseline\n","    base_pred = net.forward(X)\n","    base_loss = net.loss.loss(base_pred, Y)\n","\n","    for i in range(W.shape[0]):\n","        for j in range(W.shape[1]):\n","            old = W[i, j]\n","\n","            W[i, j] = old + eps\n","            lp = net.loss.loss(net.forward(X), Y)\n","\n","            W[i, j] = old - eps\n","            lm = net.loss.loss(net.forward(X), Y)\n","\n","            numgrad[i, j] = (lp - lm) / (2 * eps)\n","            W[i, j] = old\n","\n","    return numgrad, base_loss\n","\n","# small classification net for the check\n","Xc = np.random.randn(6, 3)\n","yc = one_hot(np.random.randint(0, 2, size=6), 2)\n","\n","netc = NeuralNetwork(in_dim=3, layers=[5], out_dim=2, loss=\"cross_entropy\")\n","\n","# analytic grad\n","pred = netc.forward(Xc)\n","loss_val = netc.loss.loss(pred, yc)\n","grads = netc.loss.gradient(pred, yc)\n","netc.backward(grads)\n","analytic = netc.layers[0].grad_weight.copy()\n","\n","# numerical grad\n","num, _ = numerical_grad_weight(netc, Xc, yc, layer_index=0, eps=1e-5)\n","\n","err = np.linalg.norm(analytic - num)\n","print(\"Loss:\", loss_val)\n","print(\"error:\", err)\n","print(\"(Rule of thumb: <1e-4 is usually good for this tiny check.)\")\n"],"metadata":{"id":"-fq3SjMefS9I"},"id":"-fq3SjMefS9I","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","df = pd.read_csv(\"/content/sample_data/california_housing_train.csv\")\n","\n","X , y = df.iloc[: , :-1].values , df.iloc[: , -1 : ].values /10000\n","\n","X = (X  - np.mean(X , axis= 0 )) / np.std(X , axis= 0 )\n","perm = np.random.permutation(X.shape[0])\n","X_shuf, y_shuf = X[perm], y[perm]\n","\n","split = int(X.shape[0] * 0.8)\n","X_train , y_train = X_shuf[:split] , y_shuf[:split]\n","X_val , y_val = X_shuf[split:] , y_shuf[split:]\n","\n","model = NeuralNetwork(X.shape[1] , [16  , 32 , 32 , 16] , y.shape[1] , \"mse\")\n","\n","model.train(X_train , y_train , 1000 , 0.001 , 64 , verbose_every=1 , x_val = X_val , y_val = y_val)\n","\n"],"metadata":{"id":"CyhuYTYafX63"},"id":"CyhuYTYafX63","execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","df = pd.read_csv(\"/content/sample_data/mnist_train_small.csv\")\n","\n","X , y = df.iloc[: ,1 : ].values , df.iloc[: , 0 ].values\n","\n","n_class = len(set(y))\n","y = one_hot(y , n_class)\n","\n","perm = np.random.permutation(X.shape[0])\n","X_shuf, y_shuf = X[perm], y[perm]\n","\n","split = int(X.shape[0] * 0.8)\n","X_train , y_train = X_shuf[:split] , y_shuf[:split]\n","X_val , y_val = X_shuf[split:] , y_shuf[split:]"],"metadata":{"id":"jHiPdc0ofZdH"},"id":"jHiPdc0ofZdH","execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = NeuralNetwork(X.shape[1] , [16  , 32 , 32 , 16] , y.shape[1] , \"cross_entropy\")\n","\n","model.train(X , y , 1000 , 0.001, 64 , verbose_every=1 , x_val = X_val , y_val = y_val)\n"],"metadata":{"id":"B4677SxafbDv"},"id":"B4677SxafbDv","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.x"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}