{"cells":[{"cell_type":"markdown","id":"ad11ae8a","metadata":{"id":"ad11ae8a"},"source":["# Backprop from scratch â€” Placeholder (Student TODO)\n","\n","Implement the `TODO` sections in order:\n","1. Losses (MSE, CrossEntropy)\n","2. Layer forward/backward\n","3. step() with **L2 regularization on weights only**\n","4. train() mini-batch SGD + validation logging\n"]},{"cell_type":"code","execution_count":5,"id":"97f9b44d","metadata":{"executionInfo":{"elapsed":39,"status":"ok","timestamp":1770396723457,"user":{"displayName":"Ahmad Aghapour","userId":"10024107043211715538"},"user_tz":300},"id":"97f9b44d"},"outputs":[],"source":["import numpy as np\n","\n","EPS = 1e-12\n","\n","def accuracy_from_logits(probs, y_onehot):\n","    pred = np.argmax(probs, axis=1)\n","    true = np.argmax(y_onehot, axis=1)\n","    return np.mean(pred == true)\n","class Loss:\n","    def loss(self, x, y):\n","        raise NotImplementedError()\n","\n","    def gradient(self, x, y):\n","        raise NotImplementedError()\n","\n","class MSE(Loss):\n","    # Mean over ALL elements (batch * out_dim)\n","    def loss(self, x, y):\n","        return np.mean(np.power(x - y, 2))\n","\n","    def gradient(self, x, y):\n","        # derivative of mean((x-y)^2) is 2*(x-y)/x.size\n","        return 2 * (x - y) / x.size\n","\n","class CrossEntropy(Loss):\n","    # Mean over samples, sum over classes: mean( -sum(y*log(p)) )\n","    def loss(self, x, y):\n","        x = np.clip(x, EPS, 1.0)\n","        return -np.mean(np.sum(y * np.log(x), axis=1))\n","\n","    def gradient(self, x, y):\n","        x = np.clip(x, EPS, 1.0)\n","        n = y.shape[0]\n","        return -(y / x) / n\n","\n","class Layer:\n","    def __init__(self, input_dim, output_dim, non_linearity=None) -> None:\n","        self.in_dim = input_dim\n","        self.out_dim = output_dim\n","        self.non_linearity = non_linearity\n","\n","        self.output = None\n","        self.input = None\n","        self.grad_weight = None\n","        self.grad_bias = None\n","\n","        # He init is fine for ReLU-ish nets\n","        self.weights = np.random.randn(self.in_dim, self.out_dim) * np.sqrt(2 / self.in_dim)\n","        self.bias = np.zeros(self.out_dim)\n","\n","    def forward(self, x):\n","        self.input = x\n","        z = x @ self.weights + self.bias\n","\n","        if self.non_linearity is None:\n","            self.output = z\n","        elif self.non_linearity == \"relu\":\n","            self.output = np.maximum(0, z)\n","        elif self.non_linearity == \"tanh\":\n","            self.output = np.tanh(z)\n","        elif self.non_linearity == \"soft_max\":\n","            # stable softmax\n","            z = z - np.max(z, axis=1, keepdims=True)\n","            exp_z = np.exp(z)\n","            self.output = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n","        else:\n","            raise ValueError(f\"Unknown non_linearity: {self.non_linearity}\")\n","\n","        return self.output\n","\n","    def backward(self, gradients):\n","        # gradients is dL/d(output_of_this_layer)\n","        if self.non_linearity is None:\n","            grad = gradients\n","        elif self.non_linearity == \"relu\":\n","            grad = (self.output > 0) * gradients\n","        elif self.non_linearity == \"tanh\":\n","            grad = (1 - self.output ** 2) * gradients\n","        elif self.non_linearity == \"soft_max\":\n","            # Jacobian-vector product for softmax\n","            # dL/dz = s * (g - sum(g*s))\n","            s = self.output\n","            grad = s * (gradients - np.sum(gradients * s, axis=1, keepdims=True))\n","        else:\n","            raise ValueError(f\"Unknown non_linearity: {self.non_linearity}\")\n","\n","        # parameter gradients\n","        self.grad_weight = self.input.T @ grad\n","        self.grad_bias = np.sum(grad, axis=0)\n","\n","        # propagate to previous layer\n","        grad_to_input = grad @ self.weights.T\n","        return grad_to_input\n","\n","class NeuralNetwork:\n","    def __init__(self, in_dim, layers, out_dim, loss) -> None:\n","        self.layers_size = layers\n","        self.in_dim = in_dim\n","        self.out_dim = out_dim\n","        self.loss = loss\n","        self.layers = []\n","        prev = self.in_dim\n","        for h in self.layers_size:\n","            self.layers.append(Layer(prev, h, non_linearity='relu'))\n","            prev = h\n","\n","        if loss == \"mse\":\n","            self.layers.append(Layer(prev, self.out_dim, non_linearity=None))\n","            self.loss = MSE()\n","        elif loss == \"cross_entropy\":\n","            self.layers.append(Layer(prev, self.out_dim, non_linearity=\"soft_max\"))\n","            self.loss = CrossEntropy()\n","        else:\n","            raise ValueError(\"loss must be 'mse' or 'cross_entropy'\")\n","\n","    def forward(self, x):\n","        for layer in self.layers:\n","            x = layer.forward(x)\n","        return x\n","\n","    def backward(self, gradients):\n","        for layer in reversed(self.layers):\n","            gradients = layer.backward(gradients)\n","\n","    def step(self, lr):\n","        for layer in self.layers:\n","            layer.weights -= lr * layer.grad_weight\n","            layer.bias -= lr * layer.grad_bias\n","\n","    def train(self, x, y, epochs, lr, batch_size=64, verbose_every=1 , x_val = None , y_val = None):\n","        n = x.shape[0]\n","\n","        for epoch in range(epochs):\n","\n","            #suffhle\n","            perm = np.random.permutation(n)\n","            x_shuf , y_shuf = x[perm] , y[perm]\n","            total_loss = 0\n","            n_batches = 0\n","            for start in range(0, n, batch_size):\n","                end = min(start + batch_size, n)\n","                xb = x_shuf[start:end]\n","                yb = y_shuf[start:end]\n","\n","               # implement training section\n","                y_pred = self.forward(xb)\n","                loss = self.loss.loss(y_pred , yb)\n","                grad_to_loss = self.loss.gradient(y_pred , yb)\n","                self.backward(grad_to_loss)\n","                self.step(lr)\n","                total_loss += loss\n","                n_batches +=1\n","\n","\n","            if (epoch + 1) % verbose_every == 0:\n","\n","                out_string = f\"Epoch {epoch+1}/{epochs} - Train_Loss: {total_loss / n_batches:.4f} \"\n","\n","                if x_val is not None  :\n","                  y_pred = self.forward(x_val)\n","                  loss = self.loss.loss(y_pred, y_val)\n","                  out_string += f\"Validation_Loss: {loss:.4f} \"\n","                  if isinstance(self.loss , CrossEntropy) :\n","                    train_accurcy = accuracy_from_logits(y_pred ,y_val )\n","                    out_string += f\"Validation_acc: {train_accurcy:.4f}\"\n","\n","                print(out_string)\n","\n","\n","\n","    def predict(self, x):\n","        return self.forward(x)\n"]},{"cell_type":"code","execution_count":6,"id":"1437d286","metadata":{"id":"1437d286","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770396725329,"user_tz":300,"elapsed":13,"user":{"displayName":"Ahmad Aghapour","userId":"10024107043211715538"}},"outputId":"7fef981e-5be4-40d1-8b16-3c429f9e10ca"},"outputs":[{"output_type":"stream","name":"stdout","text":["pred shape: (5, 3)\n","Layer 0: W (4, 8), dW (4, 8), b (8,), db (8,)\n","Layer 1: W (8, 8), dW (8, 8), b (8,), db (8,)\n","Layer 2: W (8, 3), dW (8, 3), b (3,), db (3,)\n","loss: 1.102111510650848\n"]}],"source":["# Sanity check\n","X = np.random.randn(5, 4)              # batch=5, in_dim=4\n","y = np.random.randn(5, 3)              # regression target for mse (batch=5, out_dim=3)\n","\n","net = NeuralNetwork(in_dim=4, layers=[8, 8], out_dim=3, loss=\"mse\")\n","pred = net.forward(X)\n","print(\"pred shape:\", pred.shape)\n","\n","loss_val = net.loss.loss(pred, y)\n","grads = net.loss.gradient(pred, y)\n","net.backward(grads)\n","\n","for i, layer in enumerate(net.layers):\n","    print(f\"Layer {i}: W {layer.weights.shape}, dW {layer.grad_weight.shape}, b {layer.bias.shape}, db {layer.grad_bias.shape}\")\n","print(\"loss:\", loss_val)\n"]},{"cell_type":"code","execution_count":7,"id":"-fq3SjMefS9I","metadata":{"id":"-fq3SjMefS9I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770396889139,"user_tz":300,"elapsed":30,"user":{"displayName":"Ahmad Aghapour","userId":"10024107043211715538"}},"outputId":"9913e5cb-e14f-4868-e69e-e34a36ab2e57"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loss: 0.6298078461391744\n","error: 1.6280489075489293e-11\n"]}],"source":["def one_hot(y, num_classes):\n","    y = np.asarray(y).astype(int)\n","    oh = np.zeros((y.shape[0], num_classes))\n","    oh[np.arange(y.shape[0]), y] = 1.0\n","    return oh\n","\n","\n","\n","def numerical_grad_weight(net, X, Y, layer_index=0, eps=1e-5):\n","    # compute numerical gradient for W in net.layers[layer_index]\n","    layer = net.layers[layer_index]\n","    W = layer.weights\n","    numgrad = np.zeros_like(W)\n","\n","    # baseline\n","    base_pred = net.forward(X)\n","    base_loss = net.loss.loss(base_pred, Y)\n","\n","    for i in range(W.shape[0]):\n","        for j in range(W.shape[1]):\n","            old = W[i, j]\n","\n","            W[i, j] = old + eps\n","            lp = net.loss.loss(net.forward(X), Y)\n","\n","            W[i, j] = old - eps\n","            lm = net.loss.loss(net.forward(X), Y)\n","\n","            numgrad[i, j] = (lp - lm) / (2 * eps)\n","            W[i, j] = old\n","\n","    return numgrad, base_loss\n","\n","# small classification net for the check\n","Xc = np.random.randn(6, 3)\n","yc = one_hot(np.random.randint(0, 2, size=6), 2)\n","\n","netc = NeuralNetwork(in_dim=3, layers=[5], out_dim=2, loss=\"cross_entropy\")\n","\n","# analytic grad\n","pred = netc.forward(Xc)\n","loss_val = netc.loss.loss(pred, yc)\n","grads = netc.loss.gradient(pred, yc)\n","netc.backward(grads)\n","analytic = netc.layers[0].grad_weight.copy()\n","\n","# numerical grad\n","num, _ = numerical_grad_weight(netc, Xc, yc, layer_index=0, eps=1e-5)\n","\n","err = np.linalg.norm(analytic - num)\n","print(\"Loss:\", loss_val)\n","print(\"error:\", err)\n","\n"]},{"cell_type":"code","execution_count":10,"id":"CyhuYTYafX63","metadata":{"id":"CyhuYTYafX63","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1770397254963,"user_tz":300,"elapsed":3721,"user":{"displayName":"Ahmad Aghapour","userId":"10024107043211715538"}},"outputId":"80a0d9ef-9f52-41fe-86d5-bc6eda3e9401"},"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1630844722.py:53: RuntimeWarning: overflow encountered in matmul\n","  z = x @ self.weights + self.bias\n","/tmp/ipython-input-1630844722.py:19: RuntimeWarning: overflow encountered in power\n","  return np.mean(np.power(x - y, 2))\n","/tmp/ipython-input-1630844722.py:88: RuntimeWarning: overflow encountered in matmul\n","  self.grad_weight = self.input.T @ grad\n","/tmp/ipython-input-1630844722.py:92: RuntimeWarning: overflow encountered in matmul\n","  grad_to_input = grad @ self.weights.T\n","/tmp/ipython-input-1630844722.py:76: RuntimeWarning: invalid value encountered in multiply\n","  grad = (self.output > 0) * gradients\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 2/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 3/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 4/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 5/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 6/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 7/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 8/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 9/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 10/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 11/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 12/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 13/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 14/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 15/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 16/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 17/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 18/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 19/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 20/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 21/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 22/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 23/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 24/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 25/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 26/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 27/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 28/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 29/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 30/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 31/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 32/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 33/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 34/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 35/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 36/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 37/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 38/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 39/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 40/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 41/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 42/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 43/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 44/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 45/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 46/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 47/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 48/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 49/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 50/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 51/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 52/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 53/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 54/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 55/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 56/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 57/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 58/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 59/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 60/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 61/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 62/1000 - Train_Loss: nan Validation_Loss: nan \n","Epoch 63/1000 - Train_Loss: nan Validation_Loss: nan \n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3378966292.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m16\u001b[0m  \u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"mse\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mverbose_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mx_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_val\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1630844722.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x, y, epochs, lr, batch_size, verbose_every, x_val, y_val)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m                \u001b[0;31m# implement training section\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                 \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mgrad_to_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1630844722.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1630844722.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnon_linearity\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnon_linearity\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tanh\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import pandas as pd\n","\n","df = pd.read_csv(\"/content/sample_data/california_housing_train.csv\")\n","\n","X , y = df.iloc[: , :-1].values , df.iloc[: , -1 : ].values /10000\n","\n","#X = (X  - np.mean(X , axis= 0 )) / np.std(X , axis= 0 )\n","perm = np.random.permutation(X.shape[0])\n","X_shuf, y_shuf = X[perm], y[perm]\n","\n","split = int(X.shape[0] * 0.8)\n","X_train , y_train = X_shuf[:split] , y_shuf[:split]\n","X_val , y_val = X_shuf[split:] , y_shuf[split:]\n","\n","mean =np.mean(X_train , axis = 0 )\n","std = np.std(X_train , axis = 0 )\n","\n","model = NeuralNetwork(X.shape[1] , [16  , 32 , 32 , 16] , y.shape[1] , \"mse\")\n","\n","model.train(X_train , y_train , 1000 , 0.001 , 64 , verbose_every=1 , x_val = X_val , y_val = y_val)\n","\n"]},{"cell_type":"code","execution_count":11,"id":"jHiPdc0ofZdH","metadata":{"id":"jHiPdc0ofZdH","executionInfo":{"status":"ok","timestamp":1770397265498,"user_tz":300,"elapsed":799,"user":{"displayName":"Ahmad Aghapour","userId":"10024107043211715538"}}},"outputs":[],"source":["\n","df = pd.read_csv(\"/content/sample_data/mnist_train_small.csv\")\n","\n","X , y = df.iloc[: ,1 : ].values , df.iloc[: , 0 ].values\n","\n","n_class = len(set(y))\n","y = one_hot(y , n_class)\n","\n","perm = np.random.permutation(X.shape[0])\n","X_shuf, y_shuf = X[perm], y[perm]\n","\n","split = int(X.shape[0] * 0.8)\n","X_train , y_train = X_shuf[:split] , y_shuf[:split]\n","X_val , y_val = X_shuf[split:] , y_shuf[split:]"]},{"cell_type":"code","execution_count":13,"id":"B4677SxafbDv","metadata":{"id":"B4677SxafbDv","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1770397397089,"user_tz":300,"elapsed":43809,"user":{"displayName":"Ahmad Aghapour","userId":"10024107043211715538"}},"outputId":"c04be4a5-06ea-4cbf-f5e8-523233477039"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/1000 - Train_Loss: 4.8979 Validation_Loss: 1.9264 Validation_acc: 0.2875\n","Epoch 2/1000 - Train_Loss: 1.9137 Validation_Loss: 1.8489 Validation_acc: 0.3310\n","Epoch 3/1000 - Train_Loss: 1.8410 Validation_Loss: 1.7700 Validation_acc: 0.3548\n","Epoch 4/1000 - Train_Loss: 1.8012 Validation_Loss: 1.7784 Validation_acc: 0.3362\n","Epoch 5/1000 - Train_Loss: 1.7655 Validation_Loss: 1.7043 Validation_acc: 0.3680\n","Epoch 6/1000 - Train_Loss: 1.7368 Validation_Loss: 1.7762 Validation_acc: 0.3688\n","Epoch 7/1000 - Train_Loss: 1.7188 Validation_Loss: 1.6644 Validation_acc: 0.3787\n","Epoch 8/1000 - Train_Loss: 1.7003 Validation_Loss: 1.6528 Validation_acc: 0.3830\n","Epoch 9/1000 - Train_Loss: 1.6893 Validation_Loss: 1.6551 Validation_acc: 0.3812\n","Epoch 10/1000 - Train_Loss: 1.6777 Validation_Loss: 1.6403 Validation_acc: 0.3877\n","Epoch 11/1000 - Train_Loss: 1.6661 Validation_Loss: 1.6256 Validation_acc: 0.3827\n","Epoch 12/1000 - Train_Loss: 1.6511 Validation_Loss: 1.6265 Validation_acc: 0.3857\n","Epoch 13/1000 - Train_Loss: 1.6471 Validation_Loss: 1.6240 Validation_acc: 0.3825\n","Epoch 14/1000 - Train_Loss: 1.6376 Validation_Loss: 1.6184 Validation_acc: 0.3835\n","Epoch 15/1000 - Train_Loss: 1.6298 Validation_Loss: 1.6001 Validation_acc: 0.3910\n","Epoch 16/1000 - Train_Loss: 1.6233 Validation_Loss: 1.6042 Validation_acc: 0.3860\n","Epoch 17/1000 - Train_Loss: 1.6154 Validation_Loss: 1.5901 Validation_acc: 0.3927\n","Epoch 18/1000 - Train_Loss: 1.6081 Validation_Loss: 1.5749 Validation_acc: 0.3937\n","Epoch 19/1000 - Train_Loss: 1.6008 Validation_Loss: 1.5714 Validation_acc: 0.3942\n","Epoch 20/1000 - Train_Loss: 1.5954 Validation_Loss: 1.5711 Validation_acc: 0.3915\n","Epoch 21/1000 - Train_Loss: 1.5887 Validation_Loss: 1.5633 Validation_acc: 0.3930\n","Epoch 22/1000 - Train_Loss: 1.5845 Validation_Loss: 1.5685 Validation_acc: 0.3910\n","Epoch 23/1000 - Train_Loss: 1.5788 Validation_Loss: 1.5584 Validation_acc: 0.3940\n","Epoch 24/1000 - Train_Loss: 1.5747 Validation_Loss: 1.5524 Validation_acc: 0.3965\n","Epoch 25/1000 - Train_Loss: 1.5682 Validation_Loss: 1.5738 Validation_acc: 0.3902\n","Epoch 26/1000 - Train_Loss: 1.5658 Validation_Loss: 1.5420 Validation_acc: 0.4000\n","Epoch 27/1000 - Train_Loss: 1.5572 Validation_Loss: 1.5417 Validation_acc: 0.4047\n","Epoch 28/1000 - Train_Loss: 1.5495 Validation_Loss: 1.5250 Validation_acc: 0.4080\n","Epoch 29/1000 - Train_Loss: 1.5368 Validation_Loss: 1.5179 Validation_acc: 0.4193\n","Epoch 30/1000 - Train_Loss: 1.4670 Validation_Loss: 1.4013 Validation_acc: 0.4973\n","Epoch 31/1000 - Train_Loss: 1.3814 Validation_Loss: 1.3458 Validation_acc: 0.5182\n","Epoch 32/1000 - Train_Loss: 1.3393 Validation_Loss: 1.3105 Validation_acc: 0.5312\n","Epoch 33/1000 - Train_Loss: 1.3106 Validation_Loss: 1.2819 Validation_acc: 0.5350\n","Epoch 34/1000 - Train_Loss: 1.2847 Validation_Loss: 1.2785 Validation_acc: 0.5345\n","Epoch 35/1000 - Train_Loss: 1.2667 Validation_Loss: 1.2654 Validation_acc: 0.5435\n","Epoch 36/1000 - Train_Loss: 1.2569 Validation_Loss: 1.2483 Validation_acc: 0.5430\n","Epoch 37/1000 - Train_Loss: 1.2402 Validation_Loss: 1.2266 Validation_acc: 0.5500\n","Epoch 38/1000 - Train_Loss: 1.2268 Validation_Loss: 1.2144 Validation_acc: 0.5537\n","Epoch 39/1000 - Train_Loss: 1.1987 Validation_Loss: 1.2053 Validation_acc: 0.5345\n","Epoch 40/1000 - Train_Loss: 1.1682 Validation_Loss: 1.1497 Validation_acc: 0.5940\n","Epoch 41/1000 - Train_Loss: 1.1385 Validation_Loss: 1.1144 Validation_acc: 0.6180\n","Epoch 42/1000 - Train_Loss: 1.1099 Validation_Loss: 1.0938 Validation_acc: 0.6165\n","Epoch 43/1000 - Train_Loss: 1.0886 Validation_Loss: 1.1145 Validation_acc: 0.5865\n","Epoch 44/1000 - Train_Loss: 1.0707 Validation_Loss: 1.1093 Validation_acc: 0.6030\n","Epoch 45/1000 - Train_Loss: 1.0560 Validation_Loss: 1.0407 Validation_acc: 0.6452\n","Epoch 46/1000 - Train_Loss: 1.0365 Validation_Loss: 1.0766 Validation_acc: 0.6188\n","Epoch 47/1000 - Train_Loss: 1.0219 Validation_Loss: 1.0129 Validation_acc: 0.6335\n","Epoch 48/1000 - Train_Loss: 1.0014 Validation_Loss: 1.0283 Validation_acc: 0.6370\n","Epoch 49/1000 - Train_Loss: 0.9885 Validation_Loss: 1.0548 Validation_acc: 0.6400\n","Epoch 50/1000 - Train_Loss: 0.9726 Validation_Loss: 0.9726 Validation_acc: 0.6875\n","Epoch 51/1000 - Train_Loss: 0.9591 Validation_Loss: 0.9643 Validation_acc: 0.6937\n","Epoch 52/1000 - Train_Loss: 0.9452 Validation_Loss: 0.9519 Validation_acc: 0.6947\n","Epoch 53/1000 - Train_Loss: 0.9343 Validation_Loss: 0.9332 Validation_acc: 0.7017\n","Epoch 54/1000 - Train_Loss: 0.9236 Validation_Loss: 0.9246 Validation_acc: 0.7050\n","Epoch 55/1000 - Train_Loss: 0.9085 Validation_Loss: 0.9167 Validation_acc: 0.7053\n","Epoch 56/1000 - Train_Loss: 0.9017 Validation_Loss: 0.8877 Validation_acc: 0.7268\n","Epoch 57/1000 - Train_Loss: 0.8921 Validation_Loss: 0.9129 Validation_acc: 0.7087\n","Epoch 58/1000 - Train_Loss: 0.8831 Validation_Loss: 0.9262 Validation_acc: 0.6963\n","Epoch 59/1000 - Train_Loss: 0.8718 Validation_Loss: 0.8776 Validation_acc: 0.7210\n","Epoch 60/1000 - Train_Loss: 0.8573 Validation_Loss: 0.9023 Validation_acc: 0.7143\n","Epoch 61/1000 - Train_Loss: 0.8530 Validation_Loss: 0.8981 Validation_acc: 0.7130\n","Epoch 62/1000 - Train_Loss: 0.8388 Validation_Loss: 0.9004 Validation_acc: 0.7080\n","Epoch 63/1000 - Train_Loss: 0.8226 Validation_Loss: 0.8237 Validation_acc: 0.7290\n","Epoch 64/1000 - Train_Loss: 0.8137 Validation_Loss: 1.1352 Validation_acc: 0.6737\n","Epoch 65/1000 - Train_Loss: 0.8001 Validation_Loss: 0.7915 Validation_acc: 0.7375\n","Epoch 66/1000 - Train_Loss: 0.7878 Validation_Loss: 0.8775 Validation_acc: 0.7170\n","Epoch 67/1000 - Train_Loss: 0.7790 Validation_Loss: 0.7820 Validation_acc: 0.7292\n","Epoch 68/1000 - Train_Loss: 0.7673 Validation_Loss: 0.7803 Validation_acc: 0.7512\n","Epoch 69/1000 - Train_Loss: 0.7600 Validation_Loss: 0.7675 Validation_acc: 0.7662\n","Epoch 70/1000 - Train_Loss: 0.7461 Validation_Loss: 0.7855 Validation_acc: 0.7528\n","Epoch 71/1000 - Train_Loss: 0.7416 Validation_Loss: 0.7495 Validation_acc: 0.7525\n","Epoch 72/1000 - Train_Loss: 0.7333 Validation_Loss: 0.7574 Validation_acc: 0.7490\n","Epoch 73/1000 - Train_Loss: 0.7303 Validation_Loss: 0.8232 Validation_acc: 0.7335\n","Epoch 74/1000 - Train_Loss: 0.7192 Validation_Loss: 0.7396 Validation_acc: 0.7670\n","Epoch 75/1000 - Train_Loss: 0.7129 Validation_Loss: 0.7449 Validation_acc: 0.7712\n","Epoch 76/1000 - Train_Loss: 0.7069 Validation_Loss: 0.7308 Validation_acc: 0.7715\n","Epoch 77/1000 - Train_Loss: 0.6984 Validation_Loss: 0.7472 Validation_acc: 0.7830\n","Epoch 78/1000 - Train_Loss: 0.6935 Validation_Loss: 0.6948 Validation_acc: 0.8013\n","Epoch 79/1000 - Train_Loss: 0.6856 Validation_Loss: 0.7384 Validation_acc: 0.7903\n","Epoch 80/1000 - Train_Loss: 0.6791 Validation_Loss: 0.6797 Validation_acc: 0.8067\n","Epoch 81/1000 - Train_Loss: 0.6680 Validation_Loss: 0.6704 Validation_acc: 0.8115\n","Epoch 82/1000 - Train_Loss: 0.6582 Validation_Loss: 0.6529 Validation_acc: 0.8160\n","Epoch 83/1000 - Train_Loss: 0.6493 Validation_Loss: 0.6555 Validation_acc: 0.8130\n","Epoch 84/1000 - Train_Loss: 0.6392 Validation_Loss: 0.6677 Validation_acc: 0.8050\n","Epoch 85/1000 - Train_Loss: 0.6321 Validation_Loss: 0.6418 Validation_acc: 0.8167\n","Epoch 86/1000 - Train_Loss: 0.6194 Validation_Loss: 0.6193 Validation_acc: 0.8317\n","Epoch 87/1000 - Train_Loss: 0.6127 Validation_Loss: 0.6117 Validation_acc: 0.8253\n","Epoch 88/1000 - Train_Loss: 0.6043 Validation_Loss: 0.5966 Validation_acc: 0.8273\n","Epoch 89/1000 - Train_Loss: 0.5962 Validation_Loss: 0.5947 Validation_acc: 0.8303\n","Epoch 90/1000 - Train_Loss: 0.5896 Validation_Loss: 0.6013 Validation_acc: 0.8315\n","Epoch 91/1000 - Train_Loss: 0.5804 Validation_Loss: 0.5840 Validation_acc: 0.8357\n","Epoch 92/1000 - Train_Loss: 0.5736 Validation_Loss: 0.5824 Validation_acc: 0.8393\n","Epoch 93/1000 - Train_Loss: 0.5703 Validation_Loss: 0.6087 Validation_acc: 0.8270\n","Epoch 94/1000 - Train_Loss: 0.5605 Validation_Loss: 0.5705 Validation_acc: 0.8367\n","Epoch 95/1000 - Train_Loss: 0.5527 Validation_Loss: 0.5465 Validation_acc: 0.8438\n","Epoch 96/1000 - Train_Loss: 0.5496 Validation_Loss: 0.5795 Validation_acc: 0.8310\n","Epoch 97/1000 - Train_Loss: 0.5424 Validation_Loss: 0.5656 Validation_acc: 0.8415\n","Epoch 98/1000 - Train_Loss: 0.5369 Validation_Loss: 0.5383 Validation_acc: 0.8458\n","Epoch 99/1000 - Train_Loss: 0.5271 Validation_Loss: 0.5325 Validation_acc: 0.8462\n","Epoch 100/1000 - Train_Loss: 0.5235 Validation_Loss: 0.5375 Validation_acc: 0.8417\n","Epoch 101/1000 - Train_Loss: 0.5220 Validation_Loss: 0.5251 Validation_acc: 0.8438\n","Epoch 102/1000 - Train_Loss: 0.5146 Validation_Loss: 0.5654 Validation_acc: 0.8357\n","Epoch 103/1000 - Train_Loss: 0.5070 Validation_Loss: 0.5253 Validation_acc: 0.8482\n","Epoch 104/1000 - Train_Loss: 0.5049 Validation_Loss: 0.5162 Validation_acc: 0.8575\n","Epoch 105/1000 - Train_Loss: 0.4994 Validation_Loss: 0.5116 Validation_acc: 0.8512\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-118224151.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m16\u001b[0m  \u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"cross_entropy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mverbose_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mx_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_val\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipython-input-1630844722.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x, y, epochs, lr, batch_size, verbose_every, x_val, y_val)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mn_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                 \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \u001b[0mxb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_shuf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["model = NeuralNetwork(X.shape[1] , [16  , 32 , 32 , 16] , y.shape[1] , \"cross_entropy\")\n","\n","model.train(X , y , 1000 , 0.001, 64 , verbose_every=1 , x_val = X_val , y_val = y_val)\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.x"}},"nbformat":4,"nbformat_minor":5}